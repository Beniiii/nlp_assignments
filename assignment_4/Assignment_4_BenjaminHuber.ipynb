{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 / Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minghella war der Sohn italienisch-schottische...</td>\n",
       "      <td>Anthony Minghella, CBE war ein britischer Film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ende der 1940er Jahre wurde eine erste Auteur-...</td>\n",
       "      <td>Die Auteur-Theorie ist eine Filmtheorie und di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al Pacino, geboren in Manhattan, ist der Sohn ...</td>\n",
       "      <td>Alfredo James \"Al\" Pacino ist ein US-amerikani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Der Name der Alkalimetalle leitet sich von dem...</td>\n",
       "      <td>Als Alkalimetalle werden die chemischen Elemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Die Arbeit ist bereits seit dem Altertum Gegen...</td>\n",
       "      <td>Das deutsche Arbeitsrecht ist ein Rechtsgebiet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Minghella war der Sohn italienisch-schottische...   \n",
       "1  Ende der 1940er Jahre wurde eine erste Auteur-...   \n",
       "2  Al Pacino, geboren in Manhattan, ist der Sohn ...   \n",
       "3  Der Name der Alkalimetalle leitet sich von dem...   \n",
       "4  Die Arbeit ist bereits seit dem Altertum Gegen...   \n",
       "\n",
       "                                             summary  \n",
       "0  Anthony Minghella, CBE war ein britischer Film...  \n",
       "1  Die Auteur-Theorie ist eine Filmtheorie und di...  \n",
       "2  Alfredo James \"Al\" Pacino ist ein US-amerikani...  \n",
       "3  Als Alkalimetalle werden die chemischen Elemen...  \n",
       "4  Das deutsche Arbeitsrecht ist ein Rechtsgebiet...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"Summarization_train.csv\", sep=',')\n",
    "train_data = train_data.head(10000)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractive summary creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank\n",
    "Als Hilfestellung hierfür habe ich folgendes Tutorial durchgearbeitet: https://joshbohde.com/blog/document-summarization/\n",
    "\n",
    "Für den Textrank habe ich die Methode mit dem Graphen genommen mit folgenden Schritten:\n",
    "1. Als erstes werden Texte in Sätze tokenisiert.\n",
    "2. Sätze in Wortsammlungen tokenisieren, dargestellt als Vektor, wobei jede Dimension jeweils ein Wort räpresentiert. Die resultierende Matrix besteht aus Reihen von Sätzen und Kolonnen von Wörtern.\n",
    "3. Damit die Matrix in einen Graphen umgewandelt werden kann, wird sie mithilfe des Tfidf-Transformers normalisiert, um die Wörter, die in jedem Satz vorkommen weniger zu gewichten.\n",
    "4. Die Matrix wird in einen Graphen umgewandelt, indem sie mit ihrer transponierten Version multipliziert wird. Der daraus resultierende Graph (gespiegelte Matrix) stellt die Gleichheit zwischen den Sätzen dar\n",
    "5. Abschliessend folgt das Bewerten und sortieren mittels Pagerank von NetworkX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(document):\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    sentences = sentence_tokenizer.tokenize(document)\n",
    "    vector_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    normalized_matrix = TfidfTransformer().fit_transform(vector_matrix)\n",
    "    similarity_graph = normalized_matrix * normalized_matrix.T\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
    "    ranks = nx.pagerank(nx_graph)\n",
    "    return sorted(((ranks[i],s) for i,s in enumerate(sentences)),\n",
    "                  reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries erstellen\n",
    "Für jeden Text aus den eingespiesenen Daten, wird eine eine Zusammenfassung zwischen 85 und 115 Wörter aus bestehenden Sätzen extrahiert mit folgenden Schritten:\n",
    "\n",
    "1. Der Satz mit dem höchsten Rang wird extrahiert und der Zusammenfassung hinzugefügt.\n",
    "2. Der extrahierte Satz wird aus dem Text gelöscht, damit am Ende nicht zu viel redundante Information vorhanden ist.\n",
    "3. Der Textrank wird wieder neu berechnet (ohne dem extrahierten Satz). Schritte 1 und 2 werden solange die Zusammenfassung weniger als 85 Wörter enthält wiederholt.\n",
    "4. Wird mit dem Hinzufügen eines Satzes die Maximallänge (115 Wörter) überschritten, wird dieser Satz nicht hinzugefügt und aus dem Text gelöscht, bis ein Satz gefunden wird, mit dem eine gültige anzahl Wörter erreicht werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "min_len = 85\n",
    "max_len = 115\n",
    "for i in range(len(train_data['source'])):\n",
    "    summary = []\n",
    "    length_summary = 0\n",
    "    while length_summary < min_len:\n",
    "        ranked_text = textrank(train_data['source'][i])\n",
    "        if((length_summary + len(ranked_text[0][1].split())) > max_len):\n",
    "            train_data['source'][i] = train_data['source'][i].replace(ranked_text[0][1], '')\n",
    "        else:\n",
    "            length_summary = length_summary + len(ranked_text[0][1].split())\n",
    "            summary.append(ranked_text[0][1])\n",
    "            train_data['source'][i] = train_data['source'][i].replace(ranked_text[0][1], '')  \n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier den Index ändern um entsprechende Zusammenfassungen auszugeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Auch als Produzent war er erfolgreich, darunter für die Filme \"Der stille Amerikaner\", \"Die Dolmetscherin\" und \"Der Vorleser\", für den er 2008 posthum für den Oscar nominiert wurde.', 'Der Ehe entstammen zwei Kinder, die in der Filmbranche tätig sind: Tochter Hannah Minghella in der Produktion und Sohn Max Minghella als Schauspieler .', '1997 erhielt er für \"Der englische Patient\" den Oscar in der Rubrik \"Beste Regie\", 1999 eine Oscar-Nominierung in der Kategorie \"Bestes adaptiertes Drehbuch\" für \"Der talentierte Mr.', 'Minghella war der Sohn italienisch-schottischer Eltern, die auf der Isle of Wight eine Fabrik für Eiscreme betrieben.']\n"
     ]
    }
   ],
   "source": [
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "Um den Rouge-Score berechnen zu können bilde ich aus den Zusammenfassungen Listen aus Uni- und Bigrammen. In den Folien (NLP_Woche_11.ppt Folie 32) werden die Anzahl n-Gramme jeweils als mathematische Mengen in den Formeln angegeben, daher entferne ich alle duplikate in den Listen von n-Grammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createListOfUnigrams(summary):\n",
    "    summary = summary.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    summary_words = summary.split()\n",
    "    return list(dict.fromkeys(summary_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createListOfBigrams(summary_words):\n",
    "    summary_tuples = []\n",
    "    for i in range(len(summary_words)-1):\n",
    "        list_tuple = [summary_words[i], summary_words[i+1]]\n",
    "        summary_tuples.append(tuple(list_tuple))\n",
    "    return summary_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createListsOfUniBiSum(list_summaries):\n",
    "    list_summaries_unigrams = []\n",
    "    list_summaries_bigrams = []\n",
    "    for s in list_summaries:\n",
    "        list_summaries_unigrams.append(createListOfUnigrams(s))\n",
    "    for u in list_summaries_unigrams:\n",
    "        list_summaries_bigrams.append(createListOfBigrams(u))\n",
    "    return list_summaries_unigrams, list_summaries_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['auch', 'als', 'produzent', 'war', 'er', 'erfolgreich', 'darunter', 'für', 'die', 'filme', 'der', 'stille', 'amerikaner', 'dolmetscherin', 'und', 'vorleser', 'den', '2008', 'posthum', 'oscar', 'nominiert', 'wurdeder', 'ehe', 'entstammen', 'zwei', 'kinder', 'in', 'filmbranche', 'tätig', 'sind', 'tochter', 'hannah', 'minghella', 'produktion', 'sohn', 'max', 'schauspieler', '1997', 'erhielt', 'englische', 'patient', 'rubrik', 'beste', 'regie', '1999', 'eine', 'oscarnominierung', 'kategorie', 'bestes', 'adaptiertes', 'drehbuch', 'talentierte', 'mrminghella', 'italienischschottischer', 'eltern', 'auf', 'isle', 'of', 'wight', 'fabrik', 'eiscreme', 'betrieben']\n",
      "\n",
      "[('auch', 'als'), ('als', 'produzent'), ('produzent', 'war'), ('war', 'er'), ('er', 'erfolgreich'), ('erfolgreich', 'darunter'), ('darunter', 'für'), ('für', 'die'), ('die', 'filme'), ('filme', 'der'), ('der', 'stille'), ('stille', 'amerikaner'), ('amerikaner', 'dolmetscherin'), ('dolmetscherin', 'und'), ('und', 'vorleser'), ('vorleser', 'den'), ('den', '2008'), ('2008', 'posthum'), ('posthum', 'oscar'), ('oscar', 'nominiert'), ('nominiert', 'wurdeder'), ('wurdeder', 'ehe'), ('ehe', 'entstammen'), ('entstammen', 'zwei'), ('zwei', 'kinder'), ('kinder', 'in'), ('in', 'filmbranche'), ('filmbranche', 'tätig'), ('tätig', 'sind'), ('sind', 'tochter'), ('tochter', 'hannah'), ('hannah', 'minghella'), ('minghella', 'produktion'), ('produktion', 'sohn'), ('sohn', 'max'), ('max', 'schauspieler'), ('schauspieler', '1997'), ('1997', 'erhielt'), ('erhielt', 'englische'), ('englische', 'patient'), ('patient', 'rubrik'), ('rubrik', 'beste'), ('beste', 'regie'), ('regie', '1999'), ('1999', 'eine'), ('eine', 'oscarnominierung'), ('oscarnominierung', 'kategorie'), ('kategorie', 'bestes'), ('bestes', 'adaptiertes'), ('adaptiertes', 'drehbuch'), ('drehbuch', 'talentierte'), ('talentierte', 'mrminghella'), ('mrminghella', 'italienischschottischer'), ('italienischschottischer', 'eltern'), ('eltern', 'auf'), ('auf', 'isle'), ('isle', 'of'), ('of', 'wight'), ('wight', 'fabrik'), ('fabrik', 'eiscreme'), ('eiscreme', 'betrieben')]\n",
      "\n",
      "['anthony', 'minghella', 'cbe', 'war', 'ein', 'britischer', 'filmregisseur', 'filmproduzent', 'drehbuchautor', 'dramatiker', 'hörspielautor', 'theater', 'und', 'opernregisseur']\n",
      "\n",
      "[('anthony', 'minghella'), ('minghella', 'cbe'), ('cbe', 'war'), ('war', 'ein'), ('ein', 'britischer'), ('britischer', 'filmregisseur'), ('filmregisseur', 'filmproduzent'), ('filmproduzent', 'drehbuchautor'), ('drehbuchautor', 'dramatiker'), ('dramatiker', 'hörspielautor'), ('hörspielautor', 'theater'), ('theater', 'und'), ('und', 'opernregisseur')]\n"
     ]
    }
   ],
   "source": [
    "created_summaries = []\n",
    "reference_summaries = train_data['summary']\n",
    "\n",
    "for summary in summaries:\n",
    "    text = \"\"\n",
    "    for sentence in summary:\n",
    "        text = text + sentence\n",
    "    created_summaries.append(text)\n",
    "\n",
    "created_summaries_unigrams, created_summaries_bigrams = createListsOfUniBiSum(created_summaries)\n",
    "reference_summaries_unigrams, reference_summaries_bigrams = createListsOfUniBiSum(reference_summaries)\n",
    "\n",
    "print(created_summaries_unigrams[0])\n",
    "print()\n",
    "print(created_summaries_bigrams[0])\n",
    "print()\n",
    "print(reference_summaries_unigrams[0])\n",
    "print()\n",
    "print(reference_summaries_bigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge-n Scores \n",
    "Für die Berechnung des Rouge-Scores wird in einem ersten Schritt Recall und Precision gemäss den Formeln in den Folien (NLP_Woche_11.ppt Folie 32) berechnet. Die Funktion habe ich so definiert, dass sie für beliebige Rouge-n-Scores verwendet werden kann. Im Unterricht der Woche 11 wurde empfohlen, dass man anstelle von alpha = 0.5 (Folie 33) auch den F1-Score verwenden kann/darf. Ich habe dann die Funktion mit beiden Werten (0.5 und F1) getestet und mit dem F1 bessere Rouge-Scores  (vor allem bei Rouge-2) erhalten, weshalb ich dann diese herangehensweise gewählt habe.\n",
    "Als ich dann die Funktion über alle Zusammenfassungen laufen liess, hat sich herausgestellt dass einzelne wenige einen Recall und Precision von 0 hatten, daher gebe ich direkt einen Rouge-Score von 0 zurück um einen Divide by 0 error zu verhindern, sollte dies der Fall sein.\n",
    "Abschliessend wird noch der durchschnittliche Rouge-Score über alle Zusammenfassungen berechnet und ausgegeben.\n",
    "Da die Laufzeit extrem lange wird bei mehr Texten, erstelle ich Zusammenfassungen nur aus den ersten 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRouge(created_summary_words, reference_summary_words):\n",
    "    sum_ref = len(reference_summary_words)\n",
    "    sum_crtd = len(created_summary_words)\n",
    "    sum_crtd_ref = 0\n",
    "    for w in created_summary_words:\n",
    "        if(w in reference_summary_words):\n",
    "            sum_crtd_ref = sum_crtd_ref + 1\n",
    "    r_rouge = sum_crtd_ref / sum_ref\n",
    "    p_rouge = sum_crtd_ref / sum_crtd\n",
    "    if((r_rouge == 0.0) & (p_rouge == 0.0)):\n",
    "        return r_rouge, p_rouge, 0, 0\n",
    "    else:\n",
    "        f1 = 2 * (p_rouge * r_rouge) / (p_rouge + r_rouge)\n",
    "        rouge = (r_rouge * p_rouge) / ((f1 * r_rouge) + (1-f1) * (p_rouge))\n",
    "        return r_rouge, p_rouge, f1, rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageRouge(created_summary_words, reference_summary_words):\n",
    "    recall_rouge_1 = 0\n",
    "    precision_rouge_1 = 0\n",
    "    f1_rouge_1 = 0\n",
    "    rouge_1 = 0\n",
    "    for i in range(len(created_summary_words)):\n",
    "        rc_1, pr_1, f1_1, r_1 = calcRouge(created_summary_words[i], reference_summary_words[i])\n",
    "        recall_rouge_1 = recall_rouge_1 + rc_1 \n",
    "        precision_rouge_1 = precision_rouge_1 + pr_1\n",
    "        f1_rouge_1 = f1_rouge_1 + f1_1\n",
    "        rouge_1 = rouge_1 + r_1\n",
    "    recall_rouge_1 = recall_rouge_1 / len(created_summary_words)\n",
    "    precision_rouge_1 = precision_rouge_1 / len(created_summary_words)\n",
    "    f1_rouge_1 = f1_rouge_1 / len(created_summary_words)\n",
    "    rouge_1 = rouge_1 / len(created_summary_words)\n",
    "    return recall_rouge_1, precision_rouge_1, f1_rouge_1, rouge_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe Rouge-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.36199436754721404\n",
      "precision:  0.15742786696838096\n",
      "f1:  0.2074740227995985\n",
      "rouge_1:  0.27194055503505377\n"
     ]
    }
   ],
   "source": [
    "recall_rouge_1, precision_rouge_1, f1_rouge_1, rouge_1 = averageRouge(created_summaries_unigrams, reference_summaries_unigrams)\n",
    "print(\"recall: \", recall_rouge_1)\n",
    "print(\"precision: \", precision_rouge_1)\n",
    "print(\"f1: \", f1_rouge_1)\n",
    "print(\"rouge_1: \", rouge_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe Rouge-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.03927008306845611\n",
      "precision:  0.015702284177859346\n",
      "f1:  0.02113519063846897\n",
      "rouge_2:  0.03489116789365048\n"
     ]
    }
   ],
   "source": [
    "recall_rouge_2, precision_rouge_2, f1_rouge_2, rouge_2 = averageRouge(created_summaries_bigrams, reference_summaries_bigrams)\n",
    "print(\"recall: \", recall_rouge_2)\n",
    "print(\"precision: \", precision_rouge_2)\n",
    "print(\"f1: \", f1_rouge_2)\n",
    "print(\"rouge_2: \", rouge_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
