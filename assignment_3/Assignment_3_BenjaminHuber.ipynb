{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 / POS-Tagger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Structure from https://nlpforhackers.io/lstm-pos-tagger-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import map_tag\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import backend\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open('POS_GERMAN_train.txt').read()\n",
    "train = train.replace(\"\\n\", \"\")\n",
    "train = train.replace(\" \", \"\")\n",
    "train_tagged_list = train.split(\";\")\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test = open('POS_GERMAN_minitest.txt').read()\n",
    "test = test.replace(\"\\n\", \"\")\n",
    "test = test.replace(\" \", \"\")\n",
    "test_tagged_list = test.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train_tagged_list)):\n",
    "    train_tagged_list[i]=train_tagged_list[i].rsplit(\"/\", 1)\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "    \n",
    "for i in range(0,len(test_tagged_list)):\n",
    "    test_tagged_list[i]=test_tagged_list[i].rsplit(\"/\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BONN', 'NE'], [',', '$,'], ['10.', 'ADJA'], ['Maerz', 'NN'], ['(', '$('], ['dpa', 'NE'], [')', '$('], ['.', '$.'], ['Qualifikation', 'NN'], ['und', 'KON'], ['Ausbildung', 'NN'], ['von', 'APPR'], ['Mitarbeitern', 'NN'], ['privater', 'ADJA'], ['Wachdienste', 'NN'], ['sind', 'VAFIN'], ['nach', 'APPR'], ['Expertenmeinung', 'NN'], ['derzeit', 'ADV'], ['unzureichend', 'ADJD'], ['.', '$.'], ['An', 'APPR'], ['sie', 'PPER'], [',', '$,'], ['einschliesslich', 'APPR'], ['Zuverlaessigkeit', 'NN'], ['der', 'ART'], ['Bediensteten', 'NN'], [',', '$,'], ['muessten', 'VMFIN'], ['hoehere', 'ADJA'], ['Anforderungen', 'NN'], ['gestellt', 'VVPP'], ['werden', 'VAINF'], [',', '$,'], ['verlangten', 'VVFIN'], ['zehn', 'CARD'], ['Sachverstaendige', 'NN'], ['bei', 'APPR'], ['einer', 'ART'], ['von', 'APPR'], ['der', 'ART'], ['SPD', 'NE'], ['beantragten', 'ADJA'], ['Anhoerung', 'NN'], ['des', 'ART'], ['Bundestags-Innenausschusses', 'NN'], ['am', 'APPRART'], ['Montag', 'NN'], ['in', 'APPR'], ['Bonn', 'NE'], ['.', '$.'], ['Die', 'ART'], ['Mitarbeiter', 'NN'], ['wuerden', 'VAFIN'], ['in', 'APPR'], ['Bahnhoefen', 'NN'], [',', '$,'], ['Nuklearanlagen', 'NN'], ['und', 'KON'], ['Chemiewerken', 'NN'], ['eingesetzt', 'VVPP'], ['und', 'KON'], ['seien', 'VAFIN'], ['damit', 'PAV'], ['fuer', 'APPR'], ['die', 'ART'], ['Gesundheit', 'NN'], ['Tausender', 'ADJA'], ['Menschen', 'NN'], ['verantwortlich', 'ADJD'], [',', '$,'], ['sagte', 'VVFIN'], ['Wilhelm', 'NE'], ['Zechner', 'NE'], ['vom', 'APPRART'], ['Vorstand', 'NN'], ['der', 'ART'], ['Gewerkschaft', 'NN'], ['OeTV', 'NE'], ['.', '$.'], ['Auch', 'ADV'], ['beim', 'APPRART'], ['Waffenrecht', 'NN'], ['muesse', 'VMFIN'], ['nachgezogen', 'VVPP'], ['werden', 'VAINF'], [',', '$,'], ['betonte', 'VVFIN'], ['Paul', 'NE'], ['Beinhofer', 'NE'], ['vom', 'APPRART'], ['bayerischen', 'ADJA'], ['Innenministerium', 'NN'], ['.', '$.'], ['Bei', 'APPR'], ['der', 'ART'], ['Frage', 'NN'], ['nach', 'APPR'], ['einer', 'ART']]\n"
     ]
    }
   ],
   "source": [
    "print(test_tagged_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert STTS to Universal\n",
    "Die Konvertierung ist mit Hilfe von folgender Tabelle vorgenommen worden: \n",
    "https://universaldependencies.org/tagset-conversion/de-stts-uposf.html\n",
    "\n",
    "Da hier aber vermerkt wird, dass die Tabelle fehlerhaft sein kann, habe ich die einzelnen Einträge Stück für Stück auf mögliche schwerwiegende Fehler überprüft (z.B. dass Artikel als Verben abgebildet sind). Zum Beispiel hatte ich Zweifel bei attributierenden Possessiv- und Relativpronomen, da diese nicht als Pronomen abgebildet sind. DET ist allerdigs, nachdem ich dessen Erläuterung gelesen hatte zutreffender als Pronomen.\n",
    "Zum Abschluss musste ich noch die Liste ergänzen mit SGML (markup) und SPELL (Buchstabierreihenfolge) ergänzen, da diese fehlend waren. Beide werden auf X abgebildet, da dies nicht wirklich Wortarten sind und kein anderer Tag zutreffend ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJA', 'ADJD', 'ADV', 'APPR', 'APPRART', 'APPO', 'APZR', 'ART', 'CARD', 'FM', 'ITJ', 'KOUI', 'KOUS', 'KON', 'KOKOM', 'NN', 'NE', 'PDS', 'PDAT', 'PIS', 'PIAT', 'PIDAT', 'PPER', 'PPOSS', 'PPOSAT', 'PRELS', 'PRELAT', 'PRF', 'PWS', 'PWAT', 'PWAV', 'PAV', 'PTKZU', 'PTKNEG', 'PTKVZ', 'PTKANT', 'PTKA', 'SGML', 'SPELL', 'TRUNC', 'VVFIN', 'VVIMP', 'VVINF', 'VVIZU', 'VVPP', 'VAFIN', 'VAIMP', 'VAINF', 'VAPP', 'VMFIN', 'VMINF', 'VMPP', 'XY', '$,', '$.', '$(']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#https://universaldependencies.org/tagset-conversion/de-stts-uposf.html\n",
    "tagset_conversion = {\n",
    "    'ADJA':'ADJ',\n",
    "    'ADJD':'ADJ',\n",
    "    'APPR':'ADP',\n",
    "    'APPRART':'ADP',\n",
    "    'APPO':'ADP',\n",
    "    'APZR':'ADP',\n",
    "    'ART':'DET',\n",
    "    'CARD':'NUM',\n",
    "    'FM':'X',\n",
    "    'ITJ':'INTJ',\n",
    "    'KOUI':'SCONJ',\n",
    "    'KOUS':'SCONJ',\n",
    "    'KON':'CCONJ',\n",
    "    'KOKOM':'CCONJ',\n",
    "    'NN':'NOUN',\n",
    "    'NE':'PROPN',\n",
    "    'PDS':'PRON',\n",
    "    'PDAT':'DET',\n",
    "    'PIS':'PRON',\n",
    "    'PIAT':'DET',\n",
    "    'PIDAT':'DET',\n",
    "    'PPER':'PRON',\n",
    "    'PPOSS':'PRON',\n",
    "    'PPOSAT':'DET',\n",
    "    'PRELS':'PRON',\n",
    "    'PRELAT':'DET',\n",
    "    'PRF':'PRON',\n",
    "    'PWS':'PRON',\n",
    "    'PWAT':'DET',\n",
    "    'PWAV':'ADV',\n",
    "    'PAV':'ADV',\n",
    "    'PTKZU':'PART',\n",
    "    'PTKNEG':'PART',\n",
    "    'PTKVZ':'ADP',\n",
    "    'PTKANT':'PART',\n",
    "    'PTKA':'PART',\n",
    "    'SGML': 'X',\n",
    "    'SPELL': 'X',\n",
    "    'TRUNC':'X',\n",
    "    'VVFIN':'VERB',\n",
    "    'VVIMP':'VERB',\n",
    "    'VVINF':'VERB',\n",
    "    'VVIZU':'VERB',\n",
    "    'VVPP':'VERB',\n",
    "    'VAFIN':'AUX',\n",
    "    'VAIMP':'AUX',\n",
    "    'VAINF':'AUX',\n",
    "    'VAPP':'AUX',\n",
    "    'VMFIN':'VERB',\n",
    "    'VMINF':'VERB',\n",
    "    'VMPP':'VERB',\n",
    "    'XY':'X',\n",
    "    '$,':'PUNCT',\n",
    "    '$.':'PUNCT',\n",
    "    '$(':'PUNCT',\n",
    "}\n",
    "\n",
    "STTS_tagset = [*tagset_conversion]\n",
    "STTS_tagset.insert(2, 'ADV') #This is required, else translating with dataframe doesn't work\n",
    "print(STTS_tagset)\n",
    "print('ADJA' not in STTS_tagset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_tagged_list, columns = ['word', 'tag'])\n",
    "train_data = train_data.drop(train_data[(~train_data.tag.isin(STTS_tagset))].index)\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test_data = pd.DataFrame(test_tagged_list, columns = ['word', 'tag'])\n",
    "test_data = test_data.drop(test_data[(~test_data.tag.isin(STTS_tagset))].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['``', '$('], ['Ross', 'NE'], ['Perot', 'NE'], ['waere', 'VAFIN'], ['vielleicht', 'ADV'], ['ein', 'ART'], ['praechtiger', 'ADJA'], ['Diktator', 'NN'], [\"''\", '$('], ['Konzernchefs', 'NN'], ['lehnen', 'VVFIN'], ['den', 'ART'], ['Milliardaer', 'NN'], ['als', 'APPR'], ['US-Praesidenten', 'NN'], ['ab', 'PTKVZ'], ['/', '$('], ['Texaner', 'NN'], ['gibt', 'VVFIN'], ['nur', 'ADV'], ['vage', 'ADJA'], ['Auskunft', 'NN'], ['ueber', 'APPR'], ['seine', 'PPOSAT'], ['Wirtschaftspolitik', 'NN'], ['Der', 'ART'], ['texanische', 'ADJA'], ['Milliardaer', 'NN'], ['Ross', 'NE'], ['Perot', 'NE'], ['hat', 'VAFIN'], ['das', 'ART'], ['politische', 'ADJA'], ['Establishment', 'NN'], ['in', 'APPR'], ['Washington', 'NE'], ['aufgeschreckt', 'VVPP'], ['.', '$.']]\n",
      "[['BONN', 'NE'], [',', '$,'], ['10.', 'ADJA'], ['Maerz', 'NN'], ['(', '$('], ['dpa', 'NE'], [')', '$('], ['.', '$.']]\n"
     ]
    }
   ],
   "source": [
    "train_data_list = train_data.values.tolist()\n",
    "train_sentence_endings = []\n",
    "\n",
    "for i in range(0,len(train_data_list)):\n",
    "    if(train_data_list[i][1] == \"$.\"):\n",
    "        train_sentence_endings.append(i+1)\n",
    "        \n",
    "tagged_train_sentences = [train_data_list[i : j] for i, j in zip([0] + \n",
    "          train_sentence_endings, train_sentence_endings + [None])] \n",
    "print(tagged_train_sentences[0])\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test_data_list = test_data.values.tolist()\n",
    "test_sentence_endings = []\n",
    "\n",
    "for i in range(0,len(test_data_list)):\n",
    "    if(test_data_list[i][1] == \"$.\"):\n",
    "        test_sentence_endings.append(i+1)\n",
    "        \n",
    "tagged_test_sentences = [test_data_list[i : j] for i, j in zip([0] + \n",
    "          test_sentence_endings, test_sentence_endings + [None])] \n",
    "print(tagged_test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting tagged sentences to sentences and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'Ross', 'Perot', 'waere', 'vielleicht', 'ein', 'praechtiger', 'Diktator', \"''\", 'Konzernchefs', 'lehnen', 'den', 'Milliardaer', 'als', 'US-Praesidenten', 'ab', '/', 'Texaner', 'gibt', 'nur', 'vage', 'Auskunft', 'ueber', 'seine', 'Wirtschaftspolitik', 'Der', 'texanische', 'Milliardaer', 'Ross', 'Perot', 'hat', 'das', 'politische', 'Establishment', 'in', 'Washington', 'aufgeschreckt', '.']\n",
      "['$(', 'NE', 'NE', 'VAFIN', 'ADV', 'ART', 'ADJA', 'NN', '$(', 'NN', 'VVFIN', 'ART', 'NN', 'APPR', 'NN', 'PTKVZ', '$(', 'NN', 'VVFIN', 'ADV', 'ADJA', 'NN', 'APPR', 'PPOSAT', 'NN', 'ART', 'ADJA', 'NN', 'NE', 'NE', 'VAFIN', 'ART', 'ADJA', 'NN', 'APPR', 'NE', 'VVPP', '$.']\n",
      "['BONN', ',', '10.', 'Maerz', '(', 'dpa', ')', '.']\n",
      "['NE', '$,', 'ADJA', 'NN', '$(', 'NE', '$(', '$.']\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_tags = [],[]\n",
    "for i in range(len(tagged_train_sentences)):\n",
    "    list_sentences, list_tags = [],[]\n",
    "    for k in range(len(tagged_train_sentences[i])):\n",
    "        list_sentences.append(tagged_train_sentences[i][k][0])\n",
    "        list_tags.append(tagged_train_sentences[i][k][1])\n",
    "    train_sentences.append(list_sentences)\n",
    "    train_tags.append(list_tags)\n",
    "        \n",
    "print(train_sentences[0])\n",
    "print(train_tags[0])\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test_sentences, test_tags = [],[]\n",
    "for i in range(len(tagged_test_sentences)):\n",
    "    list_sentences, list_tags = [],[]\n",
    "    for k in range(len(tagged_test_sentences[i])):\n",
    "        list_sentences.append(tagged_test_sentences[i][k][0])\n",
    "        list_tags.append(tagged_test_sentences[i][k][1])\n",
    "    test_sentences.append(list_sentences)\n",
    "    test_tags.append(list_tags)\n",
    "        \n",
    "print(test_sentences[0])\n",
    "print(test_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the STTS tags to Universal tags\n",
    "This takes a while due to inefficient processing :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PUNCT', 'PROPN', 'PROPN', 'AUX', 'ADV', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADP', 'PUNCT', 'NOUN', 'VERB', 'ADV', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'DET', 'ADJ', 'NOUN', 'PROPN', 'PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'ADP', 'PROPN', 'VERB', 'PUNCT']\n",
      "['PROPN', 'PUNCT', 'ADJ', 'NOUN', 'PUNCT', 'PROPN', 'PUNCT', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_tags)):\n",
    "    tags_dataframe = pd.DataFrame(train_tags[i], columns = [\"tag\"])\n",
    "    tags_dataframe = tags_dataframe.replace({\"tag\": tagset_conversion})\n",
    "    train_tags[i] = tags_dataframe.values.tolist()\n",
    "    for k in range(len(train_tags[i])):\n",
    "        train_tags[i][k] =  train_tags[i][k][0]\n",
    "print(train_tags[0])\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "for i in range(len(test_tags)):\n",
    "    tags_dataframe = pd.DataFrame(test_tags[i], columns = [\"tag\"])\n",
    "    tags_dataframe = tags_dataframe.replace({\"tag\": tagset_conversion})\n",
    "    test_tags[i] = tags_dataframe.values.tolist()\n",
    "    for k in range(len(test_tags[i])):\n",
    "        test_tags[i][k] =  test_tags[i][k][0]\n",
    "print(test_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, \n",
    " dev_sentences, \n",
    " train_tags, \n",
    " dev_tags) = train_test_split(train_sentences, train_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming tags and words to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "        \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "        \n",
    "word2index = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3052, 25344, 17532, 32044, 25563, 7166, 31501, 1088, 5246, 7943, 16661, 47790, 48836, 49027]\n",
      "[7943, 8137, 21444, 18326, 63087, 4398, 1, 5886, 2163, 40590, 57842, 56150, 18072, 56970, 29707, 21764, 36434, 45867, 4621, 25344, 48009, 29707, 37151, 31964, 4991, 1, 46388, 38124, 112, 40590, 32554, 4991, 33450, 25344, 5272, 30165, 17559, 40078, 36235, 49027]\n",
      "[7, 14, 12, 4, 6, 7, 12, 12, 4, 7, 1, 1, 12, 7]\n",
      "[7, 14, 15, 4, 6, 8, 8, 16, 7, 14, 4, 4, 14, 4, 11, 14, 4, 2, 4, 14, 9, 11, 4, 14, 15, 4, 15, 12, 4, 14, 4, 15, 4, 14, 12, 4, 7, 8, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, dev_sentences_X, train_tags_y, dev_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "    \n",
    "for s in dev_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    dev_sentences_X.append(s_int)\n",
    "    \n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "for s in dev_tags:\n",
    "    dev_tags_y.append([tag2index[t] for t in s])\n",
    "    \n",
    "print(train_sentences_X[0])\n",
    "print(dev_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(dev_tags_y[0])\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test_sentences_X, test_tags_y = [],[]\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "    \n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_sentences_X = pad_sequences(dev_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_tags_y = pad_sequences(dev_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "#----Test Data-----------------------------------------------------------\n",
    "\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Benja\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 128)          8196480   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 130, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 130, 17)           8721      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 130, 17)           0         \n",
      "=================================================================\n",
      "Total params: 8,993,681\n",
      "Trainable params: 8,993,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=[ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Using 5 epochs at approx. 80-90s time estimated per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23196 samples, validate on 5799 samples\n",
      "Epoch 1/5\n",
      "23196/23196 [==============================] - 82s 4ms/step - loss: 0.0065 - ignore_accuracy: 0.9874 - val_loss: 0.0212 - val_ignore_accuracy: 0.9568\n",
      "Epoch 2/5\n",
      "23196/23196 [==============================] - 81s 3ms/step - loss: 0.0051 - ignore_accuracy: 0.9904 - val_loss: 0.0212 - val_ignore_accuracy: 0.9575\n",
      "Epoch 3/5\n",
      "23196/23196 [==============================] - 84s 4ms/step - loss: 0.0039 - ignore_accuracy: 0.9931 - val_loss: 0.0221 - val_ignore_accuracy: 0.9572\n",
      "Epoch 4/5\n",
      "23196/23196 [==============================] - 87s 4ms/step - loss: 0.0029 - ignore_accuracy: 0.9950 - val_loss: 0.0229 - val_ignore_accuracy: 0.9575\n",
      "Epoch 5/5\n",
      "23196/23196 [==============================] - 97s 4ms/step - loss: 0.0022 - ignore_accuracy: 0.9965 - val_loss: 0.0232 - val_ignore_accuracy: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bab2e91cc0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7249/7249 [==============================] - 25s 3ms/step\n",
      "ignore_accuracy dev: 95.71502806761694\n"
     ]
    }
   ],
   "source": [
    "dev_scores = model.evaluate(dev_sentences_X, to_categorical(dev_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]} dev: {dev_scores[1] * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 2s 4ms/step\n",
      "ignore_accuracy test: 94.76046433120463\n"
     ]
    }
   ],
   "source": [
    "test_scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]} test: {test_scores[1] * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores on minitest\n",
    "Auf dem minitest wurden folgende Werte erzielt:\n",
    "\n",
    "acc test: 99.15750963347298\n",
    "\n",
    "ignore_accuracy test: 94.50607793944664"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
