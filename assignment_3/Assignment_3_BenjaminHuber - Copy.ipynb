{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 / POS-Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import map_tag\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import backend\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open('POS_GERMAN_train.txt').read()\n",
    "train = train.replace(\"\\n\", \"\")\n",
    "train = train.replace(\" \", \"\")\n",
    "train_tagged_list = train.split(\";\")\n",
    "\n",
    "test = open('POS_GERMAN_minitest.txt').read()\n",
    "test = test.replace(\"\\n\", \"\")\n",
    "test = test.replace(\" \", \"\")\n",
    "test_tagged_list = test.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(train_tagged_list)):\n",
    "    train_tagged_list[i]=train_tagged_list[i].rsplit(\"/\", 1)\n",
    "    \n",
    "for i in range(0,len(test_tagged_list)):\n",
    "    test_tagged_list[i]=test_tagged_list[i].rsplit(\"/\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BONN', 'NE'], [',', '$,'], ['10.', 'ADJA'], ['Maerz', 'NN'], ['(', '$('], ['dpa', 'NE'], [')', '$('], ['.', '$.'], ['Qualifikation', 'NN'], ['und', 'KON'], ['Ausbildung', 'NN'], ['von', 'APPR'], ['Mitarbeitern', 'NN'], ['privater', 'ADJA'], ['Wachdienste', 'NN'], ['sind', 'VAFIN'], ['nach', 'APPR'], ['Expertenmeinung', 'NN'], ['derzeit', 'ADV'], ['unzureichend', 'ADJD'], ['.', '$.'], ['An', 'APPR'], ['sie', 'PPER'], [',', '$,'], ['einschliesslich', 'APPR'], ['Zuverlaessigkeit', 'NN'], ['der', 'ART'], ['Bediensteten', 'NN'], [',', '$,'], ['muessten', 'VMFIN'], ['hoehere', 'ADJA'], ['Anforderungen', 'NN'], ['gestellt', 'VVPP'], ['werden', 'VAINF'], [',', '$,'], ['verlangten', 'VVFIN'], ['zehn', 'CARD'], ['Sachverstaendige', 'NN'], ['bei', 'APPR'], ['einer', 'ART'], ['von', 'APPR'], ['der', 'ART'], ['SPD', 'NE'], ['beantragten', 'ADJA'], ['Anhoerung', 'NN'], ['des', 'ART'], ['Bundestags-Innenausschusses', 'NN'], ['am', 'APPRART'], ['Montag', 'NN'], ['in', 'APPR'], ['Bonn', 'NE'], ['.', '$.'], ['Die', 'ART'], ['Mitarbeiter', 'NN'], ['wuerden', 'VAFIN'], ['in', 'APPR'], ['Bahnhoefen', 'NN'], [',', '$,'], ['Nuklearanlagen', 'NN'], ['und', 'KON'], ['Chemiewerken', 'NN'], ['eingesetzt', 'VVPP'], ['und', 'KON'], ['seien', 'VAFIN'], ['damit', 'PAV'], ['fuer', 'APPR'], ['die', 'ART'], ['Gesundheit', 'NN'], ['Tausender', 'ADJA'], ['Menschen', 'NN'], ['verantwortlich', 'ADJD'], [',', '$,'], ['sagte', 'VVFIN'], ['Wilhelm', 'NE'], ['Zechner', 'NE'], ['vom', 'APPRART'], ['Vorstand', 'NN'], ['der', 'ART'], ['Gewerkschaft', 'NN'], ['OeTV', 'NE'], ['.', '$.'], ['Auch', 'ADV'], ['beim', 'APPRART'], ['Waffenrecht', 'NN'], ['muesse', 'VMFIN'], ['nachgezogen', 'VVPP'], ['werden', 'VAINF'], [',', '$,'], ['betonte', 'VVFIN'], ['Paul', 'NE'], ['Beinhofer', 'NE'], ['vom', 'APPRART'], ['bayerischen', 'ADJA'], ['Innenministerium', 'NN'], ['.', '$.'], ['Bei', 'APPR'], ['der', 'ART'], ['Frage', 'NN'], ['nach', 'APPR'], ['einer', 'ART']]\n"
     ]
    }
   ],
   "source": [
    "print(test_tagged_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert STTS to Universal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJA', 'ADJD', 'ADV', 'APPR', 'APPRART', 'APPO', 'APZR', 'ART', 'CARD', 'FM', 'ITJ', 'KOUI', 'KOUS', 'KON', 'KOKOM', 'NN', 'NE', 'PDS', 'PDAT', 'PIS', 'PIAT', 'PIDAT', 'PPER', 'PPOSS', 'PPOSAT', 'PRELS', 'PRELAT', 'PRF', 'PWS', 'PWAT', 'PWAV', 'PAV', 'PTKZU', 'PTKNEG', 'PTKVZ', 'PTKANT', 'PTKA', 'SGML', 'SPELL', 'TRUNC', 'VVFIN', 'VVIMP', 'VVINF', 'VVIZU', 'VVPP', 'VAFIN', 'VAIMP', 'VAINF', 'VAPP', 'VMFIN', 'VMINF', 'VMPP', 'XY', '$,', '$.', '$(']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#https://universaldependencies.org/tagset-conversion/de-stts-uposf.html\n",
    "tagset_conversion = {\n",
    "    'ADJA':'ADJ',\n",
    "    'ADJD':'ADJ',\n",
    "    'APPR':'ADP',\n",
    "    'APPRART':'ADP',\n",
    "    'APPO':'ADP',\n",
    "    'APZR':'ADP',\n",
    "    'ART':'DET',\n",
    "    'CARD':'NUM',\n",
    "    'FM':'X',\n",
    "    'ITJ':'INTJ',\n",
    "    'KOUI':'SCONJ',\n",
    "    'KOUS':'SCONJ',\n",
    "    'KON':'CCONJ',\n",
    "    'KOKOM':'CCONJ',\n",
    "    'NN':'NOUN',\n",
    "    'NE':'PROPN',\n",
    "    'PDS':'PRON',\n",
    "    'PDAT':'DET',\n",
    "    'PIS':'PRON',\n",
    "    'PIAT':'DET',\n",
    "    'PIDAT':'DET',\n",
    "    'PPER':'PRON',\n",
    "    'PPOSS':'PRON',\n",
    "    'PPOSAT':'DET',\n",
    "    'PRELS':'PRON',\n",
    "    'PRELAT':'DET',\n",
    "    'PRF':'PRON',\n",
    "    'PWS':'PRON',\n",
    "    'PWAT':'DET',\n",
    "    'PWAV':'ADV',\n",
    "    'PAV':'ADV',\n",
    "    'PTKZU':'PART',\n",
    "    'PTKNEG':'PART',\n",
    "    'PTKVZ':'ADP',\n",
    "    'PTKANT':'PART',\n",
    "    'PTKA':'PART',\n",
    "    'SGML': 'X',\n",
    "    'SPELL': 'X',\n",
    "    'TRUNC':'X',\n",
    "    'VVFIN':'VERB',\n",
    "    'VVIMP':'VERB',\n",
    "    'VVINF':'VERB',\n",
    "    'VVIZU':'VERB',\n",
    "    'VVPP':'VERB',\n",
    "    'VAFIN':'AUX',\n",
    "    'VAIMP':'AUX',\n",
    "    'VAINF':'AUX',\n",
    "    'VAPP':'AUX',\n",
    "    'VMFIN':'VERB',\n",
    "    'VMINF':'VERB',\n",
    "    'VMPP':'VERB',\n",
    "    'XY':'X',\n",
    "    '$,':'PUNCT',\n",
    "    '$.':'PUNCT',\n",
    "    '$(':'PUNCT',\n",
    "}\n",
    "\n",
    "STTS_tagset = [*tagset_conversion]\n",
    "STTS_tagset.insert(2, 'ADV') #This is required, else translating with dataframe doesn't work\n",
    "print(STTS_tagset)\n",
    "print('ADJA' not in STTS_tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      word      tag\n",
      "0                       ``       $(\n",
      "1                     Ross       NE\n",
      "2                    Perot       NE\n",
      "3                    waere    VAFIN\n",
      "4               vielleicht      ADV\n",
      "5                      ein      ART\n",
      "6              praechtiger     ADJA\n",
      "7                 Diktator       NN\n",
      "8                       ''       $(\n",
      "9             Konzernchefs       NN\n",
      "10                  lehnen    VVFIN\n",
      "11                     den      ART\n",
      "12             Milliardaer       NN\n",
      "13                     als     APPR\n",
      "14         US-Praesidenten       NN\n",
      "15                      ab    PTKVZ\n",
      "16                       /       $(\n",
      "17                 Texaner       NN\n",
      "18                    gibt    VVFIN\n",
      "19                     nur      ADV\n",
      "20                    vage     ADJA\n",
      "21                Auskunft       NN\n",
      "22                   ueber     APPR\n",
      "23                   seine   PPOSAT\n",
      "24      Wirtschaftspolitik       NN\n",
      "25                     Der      ART\n",
      "26              texanische     ADJA\n",
      "27             Milliardaer       NN\n",
      "28                    Ross       NE\n",
      "29                   Perot       NE\n",
      "...                    ...      ...\n",
      "712496           kletterte    VVFIN\n",
      "712497                  im  APPRART\n",
      "712498            gleichen     ADJA\n",
      "712499               Tempo       NN\n",
      "712500                 auf     APPR\n",
      "712501                 141     CARD\n",
      "712502           Millionen       NN\n",
      "712503                Mark       NN\n",
      "712504                   .       $.\n",
      "712505                 Die      ART\n",
      "712506  Beschaeftigtenzahl       NN\n",
      "712507                fiel    VVFIN\n",
      "712508                  um     APPR\n",
      "712509                  59     CARD\n",
      "712510                 auf     APPR\n",
      "712511                6142     CARD\n",
      "712512                   .       $.\n",
      "712513           Schiesser       NE\n",
      "712514               kennt    VVFIN\n",
      "712515               keine     PIAT\n",
      "712516               Tabus       NN\n",
      "712517        Waeschefirma       NN\n",
      "712518                will    VMFIN\n",
      "712519               altes     ADJA\n",
      "712520               Image       NN\n",
      "712521             ablegen    VVINF\n",
      "712522                 und      KON\n",
      "712523                baut    VVFIN\n",
      "712524             Stellen       NN\n",
      "712525                  ab    PTKVZ\n",
      "\n",
      "[712053 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame(train_tagged_list, columns = ['word', 'tag'])\n",
    "train_data = train_data.drop(train_data[(~train_data.tag.isin(STTS_tagset))].index)\n",
    "print(train_data)\n",
    "\n",
    "test_data = pd.DataFrame(test_tagged_list, columns = ['word', 'tag'])\n",
    "test_data = test_data.drop(test_data[(~test_data.tag.isin(STTS_tagset))].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace({\"tag\": tagset_conversion})\n",
    "test_data = test_data.replace({\"tag\": tagset_conversion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, dev_set = train_test_split(train_data, test_size=0.2)\n",
    "test_set = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for w in train_set.word:\n",
    "    words.add(w.lower())\n",
    "\n",
    "\n",
    "for t in train_set.tag:\n",
    "    tags.add(t)\n",
    "\n",
    "    \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('SCONJ', 1), ('ADP', 2), ('AUX', 3), ('ADV', 4), ('PART', 5), ('VERB', 6), ('CCONJ', 7), ('INTJ', 8), ('NUM', 9), ('PUNCT', 10), ('PRON', 11), ('PROPN', 12), ('DET', 13), ('ADJ', 14), ('NOUN', 15), ('X', 16), ('-PAD-', 0)])\n"
     ]
    }
   ],
   "source": [
    "train_words_X, dev_words_X, train_tags_y, dev_tags_y = [], [], [], []\n",
    "test_words_X, test_tags_Y = [], []\n",
    " \n",
    "for w in train_set.word:\n",
    "    w_int = []\n",
    "    try:\n",
    "        w_int.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        w_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_words_X.append(w_int)\n",
    "\n",
    "for w in dev_set.word:\n",
    "    w_int = []\n",
    "    try:\n",
    "        w_int.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        w_int.append(word2index['-OOV-'])\n",
    " \n",
    "    dev_words_X.append(w_int)\n",
    "\n",
    "    \n",
    "for t in train_set.tag:\n",
    "    train_tags_y.append([tag2index[t]])\n",
    "\n",
    "    \n",
    "for t in dev_set.tag:\n",
    "    dev_tags_y.append([tag2index[t]])\n",
    "    \n",
    "\n",
    "for w in test_set.word:\n",
    "    w_int = []\n",
    "    try:\n",
    "        w_int.append(word2index[w.lower()])\n",
    "    except KeyError:\n",
    "        w_int.append(word2index['-OOV-'])\n",
    "        \n",
    "    test_words_X.append(w_int)\n",
    "    \n",
    "for t in test_set.tag:\n",
    "    test_tags_Y.append([tag2index[t]])\n",
    "    \n",
    "print(tag2index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1\n",
    "train_words_X = pad_sequences(train_words_X, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_words_X = pad_sequences(dev_words_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "dev_tags_y = pad_sequences(dev_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "test_words_X = pad_sequences(test_words_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_Y, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Benja\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 128)            8195584   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1, 512)            788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 17)             8721      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 17)             0         \n",
      "=================================================================\n",
      "Total params: 8,992,785\n",
      "Trainable params: 8,992,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print(cat_train_tags_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Benja\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Benja\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 455713 samples, validate on 113929 samples\n",
      "Epoch 1/4\n",
      "455713/455713 [==============================] - 97s 213us/step - loss: 0.3768 - acc: 0.8827 - ignore_accuracy: 0.8827 - val_loss: 0.2350 - val_acc: 0.9219 - val_ignore_accuracy: 0.9219\n",
      "Epoch 2/4\n",
      "455713/455713 [==============================] - 94s 206us/step - loss: 0.1260 - acc: 0.9575 - ignore_accuracy: 0.9575 - val_loss: 0.2340 - val_acc: 0.9179 - val_ignore_accuracy: 0.9179\n",
      "Epoch 3/4\n",
      "455713/455713 [==============================] - 94s 206us/step - loss: 0.1106 - acc: 0.9598 - ignore_accuracy: 0.9598 - val_loss: 0.2318 - val_acc: 0.9232 - val_ignore_accuracy: 0.9232\n",
      "Epoch 4/4\n",
      "455713/455713 [==============================] - 95s 208us/step - loss: 0.1059 - acc: 0.9607 - ignore_accuracy: 0.9607 - val_loss: 0.2352 - val_acc: 0.9251 - val_ignore_accuracy: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ec84aa2e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_words_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=4, validation_split=0.2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142411/142411 [==============================] - 13s 90us/step\n",
      "acc dev: 92.65295517898224\n"
     ]
    }
   ],
   "source": [
    "scores_dev = model.evaluate(dev_words_X, to_categorical(dev_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]} dev: {scores_dev[1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8674/8674 [==============================] - 1s 90us/step\n",
      "acc test: 91.64168780262855\n"
     ]
    }
   ],
   "source": [
    "scores_test = model.evaluate(test_words_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]} test: {scores_test[1] * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
